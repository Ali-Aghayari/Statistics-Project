{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d2a3aa",
   "metadata": {},
   "source": [
    "# Question 2 practical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969fc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import math\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c89be9",
   "metadata": {},
   "source": [
    "first we complete the log_likelihood function to siginificantly lower the memory consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb71f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the likelihood function is calculated in theorical problem 5\n",
    "# the code below in implementation of that formula (based on Fi and Fj)\n",
    "def log_likelihood(F, A):\n",
    "    log_likelihood = 0\n",
    "    N = A.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            prod = np.multiply(F[i], F[j])      # addition will be on Fi and Fj product \n",
    "            sum = np.multiply(np.sum(prod), -1)\n",
    "            multiply = A[i][j] + ((-1) ** A[i][j]) * math.exp(sum)    # this help addition based on sign of adj i , j\n",
    "            log_likelihood += math.log(multiply)\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cbb15",
   "metadata": {},
   "source": [
    "gradient is much simpler than log_likliehood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae83c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the formula for gradient is calculated in theorical question 6\n",
    "def gradient(F, A, u):\n",
    "    grad = []\n",
    "    N = A.shape[0]\n",
    "    element = 0 \n",
    "    for t in range(len(F[0])):\n",
    "        element = 0\n",
    "        for v in range(N):\n",
    "            prod = np.multiply(F[u], F[v])\n",
    "            sum = np.multiply(np.sum(prod), -1)\n",
    "            multiply = ((-1) ** A[u][v]) * math.exp(sum)    \n",
    "            element += np.multiply(multiply, -F[v][t])/A[u][v] + multiply  # value calculated on one step\n",
    "        grad.append(element)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8038967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step    0 logliklihood is -548.4426\n",
      "At step    1 logliklihood is -530.8824\n",
      "At step    2 logliklihood is -518.7393\n",
      "At step    3 logliklihood is -509.4227\n",
      "At step    4 logliklihood is -501.6552\n",
      "At step    5 logliklihood is -494.8239\n",
      "At step    6 logliklihood is -488.5324\n",
      "At step    7 logliklihood is -482.4930\n",
      "At step    8 logliklihood is -476.4932\n",
      "At step    9 logliklihood is -470.6375\n",
      "At step   10 logliklihood is -464.6649\n",
      "At step   11 logliklihood is -458.6752\n",
      "At step   12 logliklihood is -452.4577\n",
      "At step   13 logliklihood is -446.0508\n",
      "At step   14 logliklihood is -439.6528\n",
      "At step   15 logliklihood is -433.8107\n",
      "At step   16 logliklihood is -427.9415\n",
      "At step   17 logliklihood is -422.0983\n",
      "At step   18 logliklihood is -416.3529\n",
      "At step   19 logliklihood is -411.2450\n",
      "At step   20 logliklihood is -406.8645\n",
      "At step   21 logliklihood is -403.2256\n",
      "At step   22 logliklihood is -399.9242\n",
      "At step   23 logliklihood is -396.9064\n",
      "At step   24 logliklihood is -394.5039\n",
      "At step   25 logliklihood is -392.4493\n",
      "At step   26 logliklihood is -390.6400\n",
      "At step   27 logliklihood is -389.0261\n",
      "At step   28 logliklihood is -387.5795\n",
      "At step   29 logliklihood is -386.3095\n",
      "At step   30 logliklihood is -385.2254\n",
      "At step   31 logliklihood is -384.3325\n",
      "At step   32 logliklihood is -383.5393\n",
      "At step   33 logliklihood is -382.8260\n",
      "At step   34 logliklihood is -382.1800\n",
      "At step   35 logliklihood is -381.5941\n",
      "At step   36 logliklihood is -381.0687\n",
      "At step   37 logliklihood is -380.5916\n",
      "At step   38 logliklihood is -380.1547\n",
      "At step   39 logliklihood is -379.7527\n",
      "At step   40 logliklihood is -379.3815\n",
      "At step   41 logliklihood is -379.0376\n",
      "At step   42 logliklihood is -378.7180\n",
      "At step   43 logliklihood is -378.4202\n",
      "At step   44 logliklihood is -378.1421\n",
      "At step   45 logliklihood is -377.8817\n",
      "At step   46 logliklihood is -377.6373\n",
      "At step   47 logliklihood is -377.4073\n",
      "At step   48 logliklihood is -377.1902\n",
      "At step   49 logliklihood is -376.9850\n",
      "At step   50 logliklihood is -376.7906\n",
      "At step   51 logliklihood is -376.6073\n",
      "At step   52 logliklihood is -376.4389\n",
      "At step   53 logliklihood is -376.2824\n",
      "At step   54 logliklihood is -376.1359\n",
      "At step   55 logliklihood is -375.9984\n",
      "At step   56 logliklihood is -375.8692\n",
      "At step   57 logliklihood is -375.7474\n",
      "At step   58 logliklihood is -375.6324\n",
      "At step   59 logliklihood is -375.5238\n",
      "At step   60 logliklihood is -375.4210\n",
      "At step   61 logliklihood is -375.3235\n",
      "At step   62 logliklihood is -375.2310\n",
      "At step   63 logliklihood is -375.1430\n",
      "At step   64 logliklihood is -375.0593\n",
      "At step   65 logliklihood is -374.9796\n",
      "At step   66 logliklihood is -374.9035\n",
      "At step   67 logliklihood is -374.8308\n",
      "At step   68 logliklihood is -374.7614\n",
      "At step   69 logliklihood is -374.6949\n",
      "At step   70 logliklihood is -374.6313\n",
      "At step   71 logliklihood is -374.5702\n",
      "At step   72 logliklihood is -374.5116\n",
      "At step   73 logliklihood is -374.4552\n",
      "At step   74 logliklihood is -374.4011\n",
      "At step   75 logliklihood is -374.3489\n",
      "At step   76 logliklihood is -374.2987\n",
      "At step   77 logliklihood is -374.2502\n",
      "At step   78 logliklihood is -374.2034\n",
      "At step   79 logliklihood is -374.1583\n",
      "At step   80 logliklihood is -374.1146\n",
      "At step   81 logliklihood is -374.0723\n",
      "At step   82 logliklihood is -374.0314\n",
      "At step   83 logliklihood is -373.9917\n",
      "At step   84 logliklihood is -373.9532\n",
      "At step   85 logliklihood is -373.9159\n",
      "At step   86 logliklihood is -373.8796\n",
      "At step   87 logliklihood is -373.8443\n",
      "At step   88 logliklihood is -373.8100\n",
      "At step   89 logliklihood is -373.7766\n",
      "At step   90 logliklihood is -373.7441\n",
      "At step   91 logliklihood is -373.7123\n",
      "At step   92 logliklihood is -373.6814\n",
      "At step   93 logliklihood is -373.6511\n",
      "At step   94 logliklihood is -373.6216\n",
      "At step   95 logliklihood is -373.5927\n",
      "At step   96 logliklihood is -373.5644\n",
      "At step   97 logliklihood is -373.5368\n",
      "At step   98 logliklihood is -373.5097\n",
      "At step   99 logliklihood is -373.4831\n",
      "At step  100 logliklihood is -373.4570\n",
      "At step  101 logliklihood is -373.4314\n",
      "At step  102 logliklihood is -373.4062\n",
      "At step  103 logliklihood is -373.3815\n",
      "At step  104 logliklihood is -373.3578\n",
      "At step  105 logliklihood is -373.3358\n",
      "At step  106 logliklihood is -373.3150\n",
      "At step  107 logliklihood is -373.2949\n",
      "At step  108 logliklihood is -373.2756\n",
      "At step  109 logliklihood is -373.2569\n",
      "At step  110 logliklihood is -373.2390\n",
      "At step  111 logliklihood is -373.2216\n",
      "At step  112 logliklihood is -373.2049\n",
      "At step  113 logliklihood is -373.1888\n",
      "At step  114 logliklihood is -373.1732\n",
      "At step  115 logliklihood is -373.1582\n",
      "At step  116 logliklihood is -373.1437\n",
      "At step  117 logliklihood is -373.1297\n",
      "At step  118 logliklihood is -373.1162\n",
      "At step  119 logliklihood is -373.1032\n",
      "[[ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [False  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAGFCAYAAAAfPZ8PAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARGklEQVR4nO3de7CcdX3H8c9uTg4ndyARQiRc1IjBCooKCA7WC6XWQrHKDJXBap1i1VFQO47tiLXt1LZ26ngZO451qnXUWjJtkFEUYVCnYrUKeKlKEkFFgQSSgWBCQpKz2z8yivQEz0P2d559nt3X67/KyT6/vZy8uzn7Pd9Ov9/vB4Cx1h32AQAYPjEAQAwAEAMAIgYARAwAiBgAEDEAIGIAQMQAgIgBABEDACIGAEQMAEgyMcyL3/WjLblh/f9k0023ZeM3b83O+3el2+3miNXLc8KpT8iTzzghZ/zeMzM5NTnMYwKMvM4w9hl89apv5MNv/UR+essds37txORETnnBSXnTP786y486vIbTAYyfWmOw4Rs/zDtf9p7ceeuWR/+HO8lZLzk9f/ovr82CxQvKHw5gjNUSg3179+Vf33FFPvW36we+rQWLp/LXV701J//mkwucDICkhhjs2b0nbz//XbnxC98uersXve0lecVfXVj0NgHG1ZzGYHp6On9x/rvy9c/eNCe3f/rvPj2XX/EmP2AGGNCcfrR0/fuunrMQJMnXPnNjLj3zbdm5feecXQNgHMzZO4Ofbborf/SkS/P/b77b7WfNSbuy5uSd+Y1TH8gRR+/JoiXT2bunm21bJrLx2wvzzS8uyW3fX5C9D1Zr1ROednze+9W/yeQh8+firgCMvDmJQb/fzyvXXpo7Nt71y/9t4ZLp/M7F23L6y3bl7qlluX/PZLqdfo6YeiBrD92aZZN70uk8dBvT08kPblqYqz+2PF/69GGZ3tc5wJUecsoLnpK/u+bydDq//usAmGlOYnDl+6/OBy79yC//7yeeN50lFx+aa+55fLbvmTrgnzlm8fZc+Lgf5KXHb8jhh+xOkvT7SaeT7N7VyfoPPSbrP7wi27c98v/3f+aLT83b17053a7BaoBHo3gM7r79nrx8zeszvXc604vmZcnlK/PdqdXppJf+r/0Rxf5jzO/08von35hLnvStTHQfOlq/n+za2c1733J0vnTloUkO/A7ggjefm0v+4eXl7hDAGCgeg3e+7D354qduyANrluTe1zwhu7uTeaS/uB9ZPyceujUffPY1WbXwoR8O/+Kdwuc/eVje+5bV6fUOfLt//4XLc8oLTjr4OwEwZorG4N4t9+XC1a/O/WuWZsslT9z/N/ejDsEv9HLE1K6se/6VeeyiHQ/7L/1ect1/HJZ/vGx1+v2H336n08nhRx2aj258f6YWHnKQ1wYYL0X/cf26j/9Xdhy/OFsuOSHpdHPwIUiSbu7evTDnfuEl+emOJQ/7L51ucvYF9+bcV26d8af6/X623XlvvvhvXxng2gDjpWgMrv/fW3LXa9cmxT7R08n9ew/Jc6++MJ/84dr86nuYfj/547fdmVXHPTjzT3U7ufL9nyt0BoDRVywGN991Z645KQVD8Av7/6np7TedlRd9/qXZvW//kTudZGJ+cvGbN8/4E/1eP7d95yfZese2wmcBGE1FYnDvrl151af/M+nO1Wf899/uxp8vzynrX5nb7t//z0bdeclZ592XZcv3HvBPbbzxtjk6D8BoKbLc5i+/fH2273lwDt4VzLSnPy/nfP4P8k9nXpOzH/uTzJtInnPe9lz1kRUP+7ruvH5+/PXLcvqpd8/5mTiwc1Y9tcjtXHPnt2q5zqDnSKqdpcT9KXUW5kaTnp9re+sqfd3A7wy+e/eWXLXxltS3IaeTfpLX3HBOrrvj2CTJ2mfM/N1E3W4/u3YaPgOoYuC/LT/+nZuHsEh5/88RXvfV38qG7YfnqWf+fMZX9PvJxFCXegK0x0B/jz+4b18+veGW9Eqd5lGa7nfyxq89P1PLZr4vmd7XzZGr9wzhVADtM1AMNmzbmj3T06XOchA62XT/Ybnixyce8L+uOemBms8D0E4DxeB79zTjh7Mf/eFT0u88/N3B4mX7csya3UM6EUC7DBSDrQ/sTHfovzK6k5/tXJq9T3xoSrk7r58XXrQt8/zMAKCSgWLQ72fG8pph6HZ62XHs0of9by+62MAZQFUDxWDZ1CE1fqT0kfX7nTy4enGSpNPp56LLNueoY/3wGKCqgf4h5cTHHFHqHAPpp5O9j5lKp9PPcWt358I3bKn9DKUGhUqY7Sx1naMuTRrwqcuo3Z8mKfH90cbnZ6B3BieuaEYMkiTzOll+1N78zSduzYRVyACPykAxWDQ5mZWLFpc6y0Am+/vyvs9uzPIj9w37KACtM/Dw8HOPf1yJcwyk0+vlnGfeIQQAB2ngGLzxtDNKnGMw3U6ecvg9wz4FQGsNHIMVixZlyeRkibMM5Lmrbh/2EQBaq8jvmHvFU59W4mYOyrxOL8856vYcvWjmL6sDoJoyMTj56Zk3pEnk6X43r1l781CuDTAqivzChsMWLMilp52Rd3/thhI3V12vn2Vf2Zw/v2xlkpUHfTPj+Ln7Ninxme0SswhN+ux422Yr2rTQp0mPW52KrSL4k2ecmrUrVsz+hYV008v8Lbty+Gd+Wts1AUZVsRhMdLv50LkvzvKpBaVu8hF10s/jlt6XVR/4frp7hrVNAWB0FF1S9tglS7P+wotyxMJFJW92hnOP2ZQrnndlJnaYKwAoofjGyqOXLsv1f/iqnHXMsQVvdf+vwztyakc+eObn8u7Tr8/SSb+IDqCUOfmN/wvnz89Hz39pPvbtm/POr3x5wG1o/Ry/+L689eT/zvNW3Z6hr08AGEFzuv7l5Sc/Leed8KS840vX5zObNqTf78/yK6/72b/sPllxyM6cf9ymvG7tjVkyuXcujwkw9uZ8F9ihUwvynt9+Uf7s2c/Jv3/vu7nuR7dmw9Z7srf3qz/47eewyd15/NJ78/vHbcgLj/5RlvhnIIDa1LYY8sjFi/OG056VN5z2rOzr9XL7radm1/RE5nens2rhjiyc8MNggGHp9Ie0t7K3ec3At1HXcEipIa4SS2fadp9LqGtZz6gtBRrX4alB1fV9WNd1ru2tq/R1xT9NBED7iAEAYgCAGAAQMQAgYgBAxACAiAEAafnQWRXjOKQ1m7q2SpUaqmnKlqy67k8VBsqoytAZAJWJAQBiAIAYABAxACBiAEDEAIDUuOlsWJq0MGY2dX1GvdRMRInP/9elrse2ijYtRmnT90+dRnFWxDsDAMQAADEAIGIAQMQAgIgBABEDACIGAKThy23qGpopoUlnrWtJy2yatAymSZry/IyjcXxsLbcBoDIxAEAMABADACIGAEQMAIgYABAxACANHzoroa6hproGVZo0gFVieKrEdapo03NcxTgOT9Vl1B5bQ2cAVCYGAIgBAGIAQMQAgIgBABEDAJJMDPsAc62uz5fX9dnkcVwG06TnsIRRe35GTZtmCEryzgAAMQBADACIGAAQMQAgYgBAxACAiAEAsdwmSXOGgCzIObC6zltiWU+J+9OkIUfaz3IbACoTAwDEAAAxACBiAEDEAICIAQARAwDS8E1nJYaA6lJiUGgct6U1aYtZk15PTVHXa9IQ3fB5ZwCAGAAgBgBEDACIGAAQMQAgYgBALLeppG2fP2/T57Hb9tjOpkmvtza9DuoyjvMMltsAUJkYACAGAIgBABEDACIGAEQMAIgYAJAxGDqrS5MWxsym1FDNbPenSY9JXYNEbbs/JZ7DcVTX41biOobOAKhMDAAQAwDEAICIAQARAwAiBgBEDABIMjHsAzTBbIMbTdo8VddQU4mNUHVtlWrKMF9SZtCuSYNe47gZrIq67nOdj613BgCIAQBiAEDEAICIAQARAwAiBgBkiMttzu5eUMt1SnwGvUlLWkpo0uKNEtepS6nXQZvmWqowizBTkx4Ty20AqEwMABADAMQAgIgBABEDACIGAEQMAIihsyTNGs6ZTZMG4JoySNSUobRR1KTvjVE7S12v2+7KTdW+bo7PAUALiAEAYgCAGAAQMQAgYgBAxACAiAEAGeLQWW/zmoFvo22btOracFWXEvenruGdNg0JldKkx7bEVry2Pf4llPgeM3QGQGViAIAYACAGAEQMAIgYABAxACDJxLAuXNfnisfxM+h1zTPU9fzMxmfUD15THpemnCNp1uupzsfFOwMAxAAAMQAgYgBAxACAiAEAEQMAIgYAZIjLbc7uXjDwbTRlKU0yesNTTRrEq2uJjgUsc2fUFju1ieU2AFQmBgCIAQBiAEDEAICIAQARAwAiBgBkiJvOSigxGFVFk7YaGXw6OCUeW4/rgbXpNdmms9bNOwMAxAAAMQAgYgBAxACAiAEAEQMAMsTlNr3Nawa+jbZ9ZrhJC2NmM2ozHHUtt6miTa/JJi0oGkclXnPX9tZV+jrvDAAQAwDEAICIAQARAwAiBgBEDACIGACQIQ6dnd29oJbrlFoY05Tr1MUymJma9PzVNQxWxag9zyXUNRBb5TqGzgCoTAwAEAMAxACAiAEAEQMAIgYARAwASMs3ndXF1q9mX6dJ2jbEVWJwsEmvpzrOUUWTXvvdlZuqfd0cnwOAFhADAMQAADEAIGIAQMQAgIgBABEDAJJMDOvCozbIVdemsyYN3ozbprNSZntcSg0s1TX4VNdWvCZ9j43ia9s7AwDEAAAxACBiAEDEAICIAQARAwDS8OU2dX2OvU0LPurSpHkGZio1p9OUZS91fS9XMWqvW8ttAKhMDAAQAwDEAICIAQARAwAiBgBEDADIEIfOzu5eMIzL0jB1LSxp0yBRXYufqhjHgcu61PW6NnQGQGViAIAYACAGAEQMAIgYABAxACBiAECSiWEfgGYqMSRU1/BUU7Z1ldKkrV9VNGmgrK6NaiXUNXBZlXcGAIgBAGIAQMQAgIgBABEDACIGAMRym0qa9nng2cx23lJnbcp1mmTUFr2M2v0ZR5bbAFCZGAAgBgCIAQARAwAiBgBEDACIGAAQQ2fMoSYN65U4S11LZ+pcblPXEqOmDCi2bYiuxGvF0BkAlYkBAGIAgBgAEDEAIGIAQMQAgIgBAGn50FmThpqYO20ajKrrrFWM2vBUXdr2PM/m2t66Sl/nnQEAYgCAGAAQMQAgYgBAxACAiAEAafmcAQdW1yKREpq0dIaDM47LbZrymqxyf8wZAFCZGAAgBgCIAQARAwAiBgBEDACIGACQZGLYBxiE5TYH1qb7XNcikSZpysDSOCr1d0ZTnqOS5/DOAAAxAEAMAIgYABAxACBiAEDEAICIAQCx6YwWaMqATyl1DQW2aWNXKSXuT4nnx6YzAFpJDAAQAwDEAICIAQARAwAiBgCk5cttmqRNi3badNY61fWZ+xK3U+r5qesss12n1EKZuq5TlzrnQLwzAEAMABADACIGAEQMAIgYABAxACBiAEAavtymrgGSpiwbGcdBr6YMPTXJqN2fpF1LZ5p0nRLPc3flpmpfN/CVAGg9MQBADAAQAwAiBgBEDACIGAAQMQAgYzB0VkKTBtfq0qT73LYBKx69ugZImzR0VkWJ+2PoDIDKxAAAMQBADACIGAAQMQAgYgBAhjhn0Nu8Ztavacpn95v0mfs2adKCj7qYvSAp9zooMWt1bW9dpWt5ZwCAGAAgBgBEDACIGAAQMQAgYgBAxACANHy5TQkGxprN8NTcadJAX4klLXUp8bjVNXRWheU2AFQmBgCIAQBiAEDEAICIAQARAwAiBgAkmRj2AeaagbKD06RhvaYMLDVpiKuUEo9tm77HSj2Hdb0m63zte2cAgBgAIAYARAwAiBgAEDEAIGIAQMZguQ3N1qbP5TdpYUnblHjsmvT5/0HPkZQ5S5XrXNtbV+m2vDMAQAwAEAMAIgYARAwAiBgAEDEAIGIAQIY4dAZAc3hnAIAYACAGAEQMAIgYABAxACBiAEDEAICIAQBJ/g/h5T25xFTH4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def train(A, C, iterations=200):\n",
    "    # initialize an F\n",
    "    N = A.shape[0]\n",
    "    F = np.random.rand(N, C)\n",
    "\n",
    "    for n in range(iterations):\n",
    "        for person in range(N):\n",
    "            grad = np.array(gradient(F, A, person))\n",
    "            F[person] += 0.005 * grad  # updating F\n",
    "            F[person] = np.maximum(0.001, F[person])  # F should be nonnegative\n",
    "        ll = log_likelihood(F, A)\n",
    "        print('At step %4i logliklihood is %5.4f' % (n, ll))\n",
    "    return F\n",
    "\n",
    "\n",
    "A = np.random.rand(40, 40)\n",
    "A[0:15, 0:25] = A[0:15, 0:25] > 1 - 0.6  # connection prob people with 1 common group\n",
    "A[0:15, 25:40] = A[0:15,25:40] > 1 - 0.1  # connection prob people with no common group\n",
    "A[15:40,25:40]=A[15:40,25:40]>1-0.7 # connection prob people with 1 common group\n",
    "A[15:25, 15:25] = A[15:25, 15:25] > 1 - 0.8  # connection prob people with 2 common group\n",
    "\n",
    "# at this point there are only NANs in A\n",
    "# let's change it so we have real numbers\n",
    "for i in range(40):\n",
    "    A[i, i] = 0 # diagonals will be zero\n",
    "    for j in range(i):\n",
    "        A[i, j] = A[j, i]   # definition of A\n",
    "\n",
    "plt.imshow(A)\n",
    "delta = np.sqrt(-np.log(1 - 0.1))  # epsilon=0.1\n",
    "F=train(A, 2, iterations = 120)\n",
    "print(F>delta)\n",
    "G=nx.from_numpy_matrix (A)\n",
    "C=F>delta # groups members\n",
    "nx.draw(G,node_color=10*(C[:,0])+20*(C[:,1]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
